# =============================================================================
# Social Media Post Generation Agent System - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your actual values
# DO NOT commit .env to version control (it's in .gitignore)

# =============================================================================
# OpenRouter Configuration
# =============================================================================
# Get your API key from: https://openrouter.ai/keys
# This is used for LLM calls (Claude, GPT-4, etc.) and DALL-E image generation
OPENROUTER_API_KEY=sk-or-v1-your-key-here

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL connection string
# Format: postgresql://[user]:[password]@[host]:[port]/[database]
# For Docker Compose: postgresql://admin:secret@postgres:5432/social_media_posts
# For local development: postgresql://admin:secret@localhost:5432/social_media_posts
DATABASE_URL=postgresql://admin:secret@localhost:5432/social_media_posts

# =============================================================================
# Langfuse Observability
# =============================================================================
# Option 1: Use Langfuse Cloud (recommended for development)
# Sign up at: https://cloud.langfuse.com
LANGFUSE_PUBLIC_KEY=pk-lf-your-public-key-here
LANGFUSE_SECRET_KEY=sk-lf-your-secret-key-here
LANGFUSE_HOST=https://cloud.langfuse.com

# Option 2: Self-hosted Langfuse (if running locally)
# LANGFUSE_PUBLIC_KEY=pk-lf-local
# LANGFUSE_SECRET_KEY=sk-lf-local
# LANGFUSE_HOST=http://localhost:3000

# =============================================================================
# Application Configuration
# =============================================================================
# Environment: development, staging, production
ENVIRONMENT=development

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Image Storage Configuration
# =============================================================================
# Local storage path for generated images
# Images will be stored in: storage/images/
IMAGE_STORAGE_PATH=./storage/images

# Maximum image size in MB
MAX_IMAGE_SIZE_MB=10

# Image format: png, jpeg, webp
IMAGE_FORMAT=png

# =============================================================================
# LangGraph Checkpoint Configuration
# =============================================================================
# Checkpoint table name in PostgreSQL (for agent state persistence)
CHECKPOINT_TABLE_NAME=checkpoints

# =============================================================================
# LLM Model Configuration
# =============================================================================
# Primary model (via OpenRouter)
PRIMARY_MODEL=anthropic/claude-3.5-sonnet

# Fallback models (comma-separated, tried in order)
FALLBACK_MODELS=openai/gpt-4o,openai/gpt-3.5-turbo

# Temperature for text generation (0.0 to 2.0)
LLM_TEMPERATURE=0.7

# Maximum tokens for text generation
LLM_MAX_TOKENS=2000

# =============================================================================
# Image Generation Configuration
# =============================================================================
# Image generation model options:
# - google/gemini-2.5-flash-image (paid, high quality)
# - google/gemini-2.5-flash-image-preview:free (FREE tier!)
# - dall-e-3 (OpenAI, high quality, $0.04/image)
# - dall-e-2 (OpenAI, cheaper, $0.02/image)
IMAGE_MODEL=google/gemini-2.5-flash-image-preview:free

# Image aspect ratio for Gemini: 1:1, 3:4, 4:3, 9:16, 16:9
IMAGE_ASPECT_RATIO=1:1

# Legacy DALL-E settings (if using DALL-E models)
IMAGE_SIZE=1024x1024
IMAGE_QUALITY=standard
IMAGE_STYLE=vivid

# =============================================================================
# Evaluation Configuration
# =============================================================================
# Enable automatic evaluation after generation
AUTO_EVALUATE=true

# LLM-as-judge model for evaluation
EVALUATION_MODEL=openai/gpt-4o

# =============================================================================
# Security Configuration
# =============================================================================
# Secret key for session management (generate a random string)
# Use: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=your-secret-key-here-change-in-production

# CORS origins (comma-separated for production)
# For development: http://localhost:3000,http://localhost:8080
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# =============================================================================
# Rate Limiting (Optional)
# =============================================================================
# Maximum requests per minute per IP
RATE_LIMIT_PER_MINUTE=60

# =============================================================================
# Docker Configuration (if using docker-compose)
# =============================================================================
# PostgreSQL
POSTGRES_USER=admin
POSTGRES_PASSWORD=secret
POSTGRES_DB=social_media_posts
POSTGRES_PORT=5432
